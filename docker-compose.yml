services:
  flask:
    build: .
    env_file: .env
    depends_on:
      - litellm
    volumes:
      - ./output:/app/output
    networks:
      - backend

  ollama:
    image: ollama/ollama
    container_name: ollama
    ports:
      - "11434:11434"
    deploy:
      resources:
        limits:
          memory: 14G
    networks:
      - backend

  litellm:
    image: ghcr.io/berriai/litellm:litellm_patch_1_67_0_stable-latest
    container_name: litellm
    ports:
      - "8000:8000"
    volumes:
      - ./litellm_config.yaml:/app/litellm_config.yaml
    environment:
      - LITELLM_CONFIG_PATH=/app/litellm_config.yaml
    depends_on:
      - ollama
    networks:
      - backend


  nginx:
    image: nginx:alpine
    ports:
      - "5050:5000"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - flask
    networks:
      - backend

networks:
  backend:
    external: true
    name: backend
